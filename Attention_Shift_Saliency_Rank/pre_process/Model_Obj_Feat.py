from keras.models import *
from keras.layers import *
from fpn_network import model_utils as fpn_model_utils
from obj_sal_seg_branch import ObjectSegmentationMaskBranch
from fpn_network.PyramidROIAlign import PyramidROIAlign
from fpn_network.BatchNorm import BatchNorm
from fpn_network import ResNet


def generate_backbone_features(input_image, config):
    if callable(config.BACKBONE):
        _, C2, C3, C4, C5 = config.BACKBONE(input_image, stage5=True,
                                            train_bn=config.TRAIN_BN)
    else:
        _, C2, C3, C4, C5 = ResNet.resnet_graph(input_image, config.BACKBONE,
                                                stage5=True, train_bn=config.TRAIN_BN)

        # Top-down Layers
    P5 = Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c5p5')(C5)
    P4 = Add(name="fpn_p4add")([UpSampling2D(size=(2, 2), name="fpn_p5upsampled")(P5),
                                Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c4p4')(C4)])
    P3 = Add(name="fpn_p3add")([UpSampling2D(size=(2, 2), name="fpn_p4upsampled")(P4),
                                Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c3p3')(C3)])
    P2 = Add(name="fpn_p2add")([UpSampling2D(size=(2, 2), name="fpn_p3upsampled")(P3),
                                Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c2p2')(C2)])

    # Attach 3x3 conv to all P layers to get the final feature maps.
    P2 = Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding="SAME", name="fpn_p2")(P2)
    P3 = Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding="SAME", name="fpn_p3")(P3)
    P4 = Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding="SAME", name="fpn_p4")(P4)
    P5 = Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding="SAME", name="fpn_p5")(P5)
    # P6 is used for the 5th anchor scale in RPN. Generated by
    # subsampling from P5 with stride of 2.
    P6 = MaxPooling2D(pool_size=(1, 1), strides=2, name="fpn_p6")(P5)

    # Note that P6 is used in RPN, but not in the classifier heads.
    # rpn_feature_maps = [P2, P3, P4, P5, P6]
    feat_pyr_net_feature_maps = [P2, P3, P4, P5]

    return feat_pyr_net_feature_maps


def pyr_roi_align_graph(rois, feature_maps, image_meta, pool_size, train_bn=True, fc_layers_size=1024):
    """Builds the computation graph of the feature pyramid network classifier
    and regressor heads.

    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized
          coordinates.
    feature_maps: List of feature maps from different layers of the pyramid,
                  [P2, P3, P4, P5]. Each has a different resolution.
    - image_meta: [batch, (meta data)] Image details. See compose_image_meta()
    pool_size: The width of the square feature map generated from ROI Pooling.
    num_classes: number of classes, which determines the depth of the results
    train_bn: Boolean. Train or freeze Batch Norm layers
    fc_layers_size: Size of the 2 FC layers

    Returns:
        pooled features
    """

    # ROI Pooling
    # Shape: [batch, num_boxes, pool_height, pool_width, channels]
    x = PyramidROIAlign([pool_size, pool_size],
                        name="roi_align_classifier")([rois, image_meta] + feature_maps)

    # Two 1024 FC layers (implemented with Conv2D for consistency)
    x = TimeDistributed(Conv2D(fc_layers_size, (pool_size, pool_size), padding="valid"), name="mrcnn_class_conv1")(x)
    x = TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn)
    x = Activation('relu')(x)

    x = TimeDistributed(Conv2D(fc_layers_size, (1, 1)), name="mrcnn_class_conv2")(x)
    x = TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn)
    x = Activation('relu')(x)

    return x


def build_obj_feat_model(config):
    # *********************** INPUTS ***********************
    input_image = Input(shape=(config.NET_IMAGE_SIZE, config.NET_IMAGE_SIZE, 3), name="input_image")
    input_image_meta = Input(shape=[config.IMAGE_META_SIZE], name="input_image_meta")

    input_obj_rois = Input(shape=(config.SAL_OBJ_NUM, 4), name="input_obj_rois")
    # Normalize coordinates
    obj_rois = Lambda(lambda x: fpn_model_utils.norm_boxes_graph(x, K.shape(input_image)[1:3]))(input_obj_rois)

    # *********************** BACKBONE FEATURES ***********************
    # Generate Backbone features
    # backbone_feat = [P2, P3, P4, P5]
    # rpn_features = [P2, P3, P4, P5, P6]
    # P2: (?, 256, 256, 256)
    # P3: (?, 128, 128, 256)
    # P4: (?, 64, 64, 256)
    # P5: (?, 32, 32, 256)
    backbone_feat = generate_backbone_features(input_image, config)

    P2, P3, P4, P5 = backbone_feat

    # *********************** SALIENT OBJECT MASK BRANCH ***********************
    # Produce Object Segment Masks
    obj_seg_masks = ObjectSegmentationMaskBranch.build_fpn_mask_graph(obj_rois, backbone_feat,
                                                                      input_image_meta,
                                                                      config.MASK_POOL_SIZE,
                                                                      config.NUM_CLASSES,
                                                                      train_bn=config.TRAIN_BN)

    # ROIAlign ed Object Features
    obj_features = pyr_roi_align_graph(obj_rois, backbone_feat, input_image_meta,
                                       config.POOL_SIZE,
                                       train_bn=config.TRAIN_BN,
                                       fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)

    # Model
    inputs = [input_image, input_image_meta, input_obj_rois]
    outputs = [obj_seg_masks, obj_features, P5]
    model = Model(inputs=inputs, outputs=outputs, name="obj_feat_model")

    return model
