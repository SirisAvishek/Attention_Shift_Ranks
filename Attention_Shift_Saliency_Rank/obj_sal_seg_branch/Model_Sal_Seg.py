from keras.models import *
from keras.layers import *
from fpn_network import model_utils
from fpn_network import utils
from fpn_network import ResNet
from fpn_network import Loss_Functions
from fpn_network import RegionProposalNetwork
from fpn_network import FeaturePyramidNetwork
from fpn_network.ProposalLayer import ProposalLayer
from fpn_network.DetectionLayer import DetectionLayer
import tensorflow as tf
from obj_sal_seg_branch import Losses
from obj_sal_seg_branch import ObjectSegmentationMaskBranch
from obj_sal_seg_branch.DetectionTargetLayer import DetectionTargetLayer


def generate_backbone_features(input_image, config):
    if callable(config.BACKBONE):
        _, C2, C3, C4, C5 = config.BACKBONE(input_image, stage5=True,
                                            train_bn=config.TRAIN_BN)
    else:
        _, C2, C3, C4, C5 = ResNet.resnet_graph(input_image, config.BACKBONE,
                                                stage5=True, train_bn=config.TRAIN_BN)

        # Top-down Layers
    P5 = Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c5p5')(C5)
    P4 = Add(name="fpn_p4add")([UpSampling2D(size=(2, 2), name="fpn_p5upsampled")(P5),
                                Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c4p4')(C4)])
    P3 = Add(name="fpn_p3add")([UpSampling2D(size=(2, 2), name="fpn_p4upsampled")(P4),
                                Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c3p3')(C3)])
    P2 = Add(name="fpn_p2add")([UpSampling2D(size=(2, 2), name="fpn_p3upsampled")(P3),
                                Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c2p2')(C2)])

    # Attach 3x3 conv to all P layers to get the final feature maps.
    P2 = Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding="SAME", name="fpn_p2")(P2)
    P3 = Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding="SAME", name="fpn_p3")(P3)
    P4 = Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding="SAME", name="fpn_p4")(P4)
    P5 = Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding="SAME", name="fpn_p5")(P5)
    # P6 is used for the 5th anchor scale in RPN. Generated by
    # subsampling from P5 with stride of 2.
    P6 = MaxPooling2D(pool_size=(1, 1), strides=2, name="fpn_p6")(P5)

    # Note that P6 is used in RPN, but not in the classifier heads.
    rpn_feature_maps = [P2, P3, P4, P5, P6]
    feat_pyr_net_feature_maps = [P2, P3, P4, P5]

    return feat_pyr_net_feature_maps, rpn_feature_maps


def get_anchors(config, image_shape):
    """Returns anchor pyramid for the given image size."""
    backbone_shapes = model_utils.compute_backbone_shapes(config, image_shape)

    _anchor_cache = {}

    # Generate Anchors
    a = utils.generate_pyramid_anchors(
        config.RPN_ANCHOR_SCALES,
        config.RPN_ANCHOR_RATIOS,
        backbone_shapes,
        config.BACKBONE_STRIDES,
        config.RPN_ANCHOR_STRIDE)

    # Normalize coordinates
    _anchor_cache[tuple(image_shape)] = utils.norm_boxes(a, image_shape[:2])

    return _anchor_cache[tuple(image_shape)]


def build_saliency_seg_model(config, mode):
    # resnet_arch = "resnet101"
    # use_stage_5 = True
    # train_bn = False, Using small batch size
    # USE_RPN_ROIS = True

    assert mode in ['training', 'inference']

    # *********************** INPUTS ***********************
    input_image = Input(shape=(None, None, 3), name="input_image")
    input_image_meta = Input(shape=[config.IMAGE_META_SIZE], name="input_image_meta")

    if mode == "training":
        # RPN GT
        input_rpn_match = Input(
            shape=[None, 1], name="input_rpn_match", dtype=tf.int32)
        input_rpn_bbox = Input(
            shape=[None, 4], name="input_rpn_bbox", dtype=tf.float32)

        # Detection GT (class IDs, bounding boxes, and masks)
        # 1. GT Class IDs (zero padded)
        input_gt_class_ids = Input(
            shape=[None], name="input_gt_class_ids", dtype=tf.int32)
        # 2. GT Boxes in pixels (zero padded)
        # [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in image coordinates
        input_gt_boxes = Input(
            shape=[None, 4], name="input_gt_boxes", dtype=tf.float32)
        # Normalize coordinates
        gt_boxes = Lambda(lambda x: model_utils.norm_boxes_graph(
            x, K.shape(input_image)[1:3]))(input_gt_boxes)
        # 3. GT Masks (zero padded)
        # [batch, height, width, MAX_GT_INSTANCES]
        if config.USE_MINI_MASK:
            input_gt_masks = Input(
                shape=[config.MINI_MASK_SHAPE[0],
                       config.MINI_MASK_SHAPE[1], None],
                name="input_gt_masks", dtype=bool)
        else:
            input_gt_masks = Input(
                shape=[config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1], None],
                name="input_gt_masks", dtype=bool)
    elif mode == "inference":
        # Anchors in normalized coordinates
        input_anchors = Input(shape=[None, 4], name="input_anchors")

    # *********************** BACKBONE FEATURES ***********************
    # Generate Backbone features
    # backbone_feat = [P2, P3, P4, P5]
    # rpn_features = [P2, P3, P4, P5, P6]
    # P2: (?, 256, 256, 256)
    # P3: (?, 128, 128, 256)
    # P4: (?, 64, 64, 256)
    # P5: (?, 32, 32, 256)
    backbone_feat, rpn_feature_maps = generate_backbone_features(input_image, config)

    # *********************** RPN ***********************
    # Anchors
    if mode == "training":
        anchors = get_anchors(config, config.IMAGE_SHAPE)
        # Duplicate across the batch dimension because Keras requires it
        anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)
        # A hack to get around Keras's bad support for constants
        anchors = Lambda(lambda x: tf.Variable(anchors), name="anchors")(input_image)
    else:
        anchors = input_anchors

    # RPN Model
    rpn = RegionProposalNetwork.build_rpn_model(config.RPN_ANCHOR_STRIDE,
                                                len(config.RPN_ANCHOR_RATIOS), config.TOP_DOWN_PYRAMID_SIZE)
    # Loop through pyramid layers
    layer_outputs = []  # list of lists
    for p in rpn_feature_maps:
        layer_outputs.append(rpn([p]))
    # Concatenate layer outputs
    # Convert from list of lists of level outputs to list of lists
    # of outputs across levels.
    # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]
    output_names = ["rpn_class_logits", "rpn_class", "rpn_bbox"]
    outputs = list(zip(*layer_outputs))
    outputs = [Concatenate(axis=1, name=n)(list(o))
               for o, n in zip(outputs, output_names)]

    rpn_class_logits, rpn_class, rpn_bbox = outputs

    # Generate proposals
    # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates
    # and zero padded.
    proposal_count = config.POST_NMS_ROIS_TRAINING if mode == "training" \
        else config.POST_NMS_ROIS_INFERENCE
    rpn_rois = ProposalLayer(
        proposal_count=proposal_count,
        nms_threshold=config.RPN_NMS_THRESHOLD,
        name="ROI",
        config=config)([rpn_class, rpn_bbox, anchors])

    # *********************** MODEL ***********************
    if mode == "training":
        # Class ID mask to mark class IDs supported by the dataset the image
        # came from.
        active_class_ids = Lambda(
            lambda x: model_utils.parse_image_meta_graph(x)["active_class_ids"]
        )(input_image_meta)

        if not config.USE_RPN_ROIS:
            # Ignore predicted ROIs and use ROIs provided as an input.
            input_rois = Input(shape=[config.POST_NMS_ROIS_TRAINING, 4], name="input_roi", dtype=np.int32)
            # Normalize coordinates
            target_rois = Lambda(lambda x: model_utils.norm_boxes_graph(x, K.shape(input_image)[1:3]))(input_rois)
        else:
            target_rois = rpn_rois

        # Generate detection targets
        # Subsamples proposals and generates target outputs for training
        # Note that proposal class IDs, gt_boxes, and gt_masks are zero
        # padded. Equally, returned rois and targets are zero padded.
        rois, target_class_ids, target_bbox, target_mask = \
            DetectionTargetLayer(config, name="proposal_targets")([
                target_rois, input_gt_class_ids, gt_boxes, input_gt_masks])

        # *********************** Network Heads
        feat_pyr_net_class_logits, feat_pyr_net_class, feat_pyr_net_bbox = \
            FeaturePyramidNetwork.fpn_classifier_graph(rois, backbone_feat, input_image_meta,
                                                       config.POOL_SIZE, config.NUM_CLASSES,
                                                       train_bn=config.TRAIN_BN,
                                                       fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)

        obj_seg_masks = ObjectSegmentationMaskBranch.build_fpn_mask_graph(rois, backbone_feat,
                                                                          input_image_meta,
                                                                          config.MASK_POOL_SIZE,
                                                                          config.NUM_CLASSES,
                                                                          train_bn=config.TRAIN_BN)

        output_rois = Lambda(lambda x: x * 1, name="output_rois")(rois)

        # *********************** Losses
        # RPN Losses
        rpn_class_loss = Lambda(lambda x: Loss_Functions.rpn_class_loss_graph(*x), name="rpn_class_loss")(
            [input_rpn_match, rpn_class_logits])
        rpn_bbox_loss = Lambda(lambda x: Loss_Functions.rpn_bbox_loss_graph(config, *x), name="rpn_bbox_loss")(
            [input_rpn_bbox, input_rpn_match, rpn_bbox])

        # Salient Object Losses
        obj_sal_seg_class_loss = Lambda(lambda x: Loss_Functions.feat_pyr_net_class_loss_graph(*x),
                                        name="obj_sal_seg_class_loss")(
            [target_class_ids, feat_pyr_net_class_logits, active_class_ids])
        obj_sal_seg_bbox_loss = Lambda(lambda x: Loss_Functions.feat_pyr_net_bbox_loss_graph(*x),
                                       name="obj_sal_seg_bbox_loss")([target_bbox, target_class_ids, feat_pyr_net_bbox])

        # Salient Object Segmentation Mask Loss
        obj_sal_seg_mask_loss = Lambda(lambda x: Losses.mask_loss_graph(*x), name="obj_sal_seg_mask_loss")(
            [target_mask, target_class_ids, obj_seg_masks])

        # *********************** FINAL MODEL
        # Model
        inputs = [input_image, input_image_meta,
                  input_rpn_match, input_rpn_bbox, input_gt_class_ids, input_gt_boxes, input_gt_masks]
        if not config.USE_RPN_ROIS:
            inputs.append(input_rois)
        outputs = [rpn_class_logits, rpn_class, rpn_bbox,
                   feat_pyr_net_class_logits, feat_pyr_net_class, feat_pyr_net_bbox, obj_seg_masks,
                   rpn_rois, output_rois,
                   rpn_class_loss, rpn_bbox_loss,
                   obj_sal_seg_class_loss, obj_sal_seg_bbox_loss,
                   obj_sal_seg_mask_loss]

        model = Model(inputs=inputs, outputs=outputs, name="obj_sal_seg_model")
    else:
        # *********************** Network Heads
        # Proposal classifier and BBox regressor heads
        feat_pyr_net_class_logits, feat_pyr_net_class, feat_pyr_net_bbox = \
            FeaturePyramidNetwork.fpn_classifier_graph(rpn_rois, backbone_feat, input_image_meta,
                                                       config.POOL_SIZE, config.NUM_CLASSES,
                                                       train_bn=config.TRAIN_BN,
                                                       fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)

        # Detections
        # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in
        # normalized coordinates
        detections = DetectionLayer(config, name="feat_pyr_net_detection")(
            [rpn_rois, feat_pyr_net_class, feat_pyr_net_bbox, input_image_meta])

        # Create masks for detections
        detection_boxes = Lambda(lambda x: x[..., :4])(detections)
        obj_seg_masks = ObjectSegmentationMaskBranch.build_fpn_mask_graph(detection_boxes, backbone_feat,
                                                                          input_image_meta,
                                                                          config.MASK_POOL_SIZE,
                                                                          config.NUM_CLASSES,
                                                                          train_bn=config.TRAIN_BN)

        model = Model(inputs=[input_image, input_image_meta, input_anchors],
                      outputs=[detections, feat_pyr_net_class, feat_pyr_net_bbox,
                               obj_seg_masks,
                               rpn_rois, rpn_class, rpn_bbox],
                      name="obj_sal_seg_model")

    return model
