import tensorflow.keras.layers as KL
from model import ResNet

import tensorflow.keras.backend as K
from model import model_utils
from model import utils

from model.BatchNorm import BatchNorm
from model.PyramidROIAlign import PyramidROIAlign

from model.AttentionLayer import AttentionLayer


def generate_backbone_features(input_image, config):
    if callable(config.BACKBONE):
        _, C2, C3, C4, C5 = config.BACKBONE(input_image, stage5=True,
                                            train_bn=config.TRAIN_BN)
    else:
        _, C2, C3, C4, C5 = ResNet.resnet_graph(input_image, config.BACKBONE,
                                                stage5=True, train_bn=config.TRAIN_BN)

    # Top-down Layers
    # TODO: add assert to varify feature map sizes match what's in config
    P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c5p5')(C5)
    P4 = KL.Add(name="fpn_p4add")([
        KL.UpSampling2D(size=(2, 2), name="fpn_p5upsampled")(P5),
        KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c4p4')(C4)])
    P3 = KL.Add(name="fpn_p3add")([
        KL.UpSampling2D(size=(2, 2), name="fpn_p4upsampled")(P4),
        KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c3p3')(C3)])
    P2 = KL.Add(name="fpn_p2add")([
        KL.UpSampling2D(size=(2, 2), name="fpn_p3upsampled")(P3),
        KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c2p2')(C2)])
    # Attach 3x3 conv to all P layers to get the final feature maps.
    P2 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding="SAME", name="fpn_p2")(P2)
    P3 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding="SAME", name="fpn_p3")(P3)
    P4 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding="SAME", name="fpn_p4")(P4)
    P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding="SAME", name="fpn_p5")(P5)
    # P6 is used for the 5th anchor scale in RPN. Generated by
    # subsampling from P5 with stride of 2.
    P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name="fpn_p6")(P5)

    # Note that P6 is used in RPN, but not in the classifier heads.
    rpn_feature_maps = [P2, P3, P4, P5, P6]
    feat_pyr_net_feature_maps = [P2, P3, P4, P5]

    return feat_pyr_net_feature_maps, rpn_feature_maps, P5


def get_anchors(config, image_shape):
    """Returns anchor pyramid for the given image size."""
    backbone_shapes = model_utils.compute_backbone_shapes(config, image_shape)

    _anchor_cache = {}

    # Generate Anchors
    a = utils.generate_pyramid_anchors(
        config.RPN_ANCHOR_SCALES,
        config.RPN_ANCHOR_RATIOS,
        backbone_shapes,
        config.BACKBONE_STRIDES,
        config.RPN_ANCHOR_STRIDE)

    # Normalize coordinates
    _anchor_cache[tuple(image_shape)] = utils.norm_boxes(a, image_shape[:2])

    return _anchor_cache[tuple(image_shape)]


def fpn_classifier_graph(rois, feature_maps, image_meta,
                         pool_size, num_classes, train_bn=True,
                         fc_layers_size=1024):
    """Builds the computation graph of the feature pyramid network classifier
    and regressor heads.

    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized
          coordinates.
    feature_maps: List of feature maps from different layers of the pyramid,
                  [P2, P3, P4, P5]. Each has a different resolution.
    - image_meta: [batch, (meta data)] Image details. See compose_image_meta()
    pool_size: The width of the square feature map generated from ROI Pooling.
    num_classes: number of classes, which determines the depth of the results
    train_bn: Boolean. Train or freeze Batch Norm layers
    fc_layers_size: Size of the 2 FC layers

    Returns:
        logits: [N, NUM_CLASSES] classifier logits (before softmax)
        probs: [N, NUM_CLASSES] classifier probabilities
        bbox_deltas: [N, (dy, dx, log(dh), log(dw))] Deltas to apply to
                     proposal boxes
    """
    # ROI Pooling
    # Shape: [batch, num_boxes, pool_height, pool_width, channels]
    x = PyramidROIAlign([pool_size, pool_size],
                        name="roi_align_classifier")([rois, image_meta] + feature_maps)

    # Two 1024 FC layers (implemented with Conv2D for consistency)
    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding="valid"),
                           name="mrcnn_class_conv1")(x)
    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn)
    x = KL.Activation('relu')(x)
    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (1, 1)),
                           name="mrcnn_class_conv2")(x)
    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn)
    object_features = KL.Activation('relu')(x)

    shared = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2),
                       name="pool_squeeze")(object_features)

    # Classifier head
    mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes),
                                            name='mrcnn_class_logits')(shared)
    mrcnn_probs = KL.TimeDistributed(KL.Activation("softmax"),
                                     name="mrcnn_class")(mrcnn_class_logits)

    # BBox head
    # [batch, boxes, num_classes * (dy, dx, log(dh), log(dw))]
    x = KL.TimeDistributed(KL.Dense(num_classes * 4, activation='linear'),
                           name='mrcnn_bbox_fc')(shared)
    # Reshape to [batch, boxes, num_classes, (dy, dx, log(dh), log(dw))]
    s = K.int_shape(x)
    if s[1] is None:
        mrcnn_bbox = KL.Reshape((-1, num_classes, 4), name="mrcnn_bbox")(x)
    else:
        mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name="mrcnn_bbox")(x)

    return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox, object_features


# --------------------------------------------------------------------------------

def mask_and_edge_network(rois, feature_maps, image_meta, pool_size, num_classes, train_bn=True):
    """Builds the computation graph of the mask head of Feature Pyramid Network.

    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized
          coordinates.
    feature_maps: List of feature maps from different layers of the pyramid,
                  [P2, P3, P4, P5]. Each has a different resolution.
    image_meta: [batch, (meta data)] Image details. See compose_image_meta()
    pool_size: The width of the square feature map generated from ROI Pooling.
    num_classes: number of classes, which determines the depth of the results
    train_bn: Boolean. Train or freeze Batch Norm layers

    Returns: Masks [batch, roi_count, height, width, num_classes]
    """
    # ROI Pooling
    # Shape: [batch, num_rois, MASK_POOL_SIZE, MASK_POOL_SIZE, channels]
    roi_align_feat = PyramidROIAlign([pool_size, pool_size], name="roi_align_mask")([rois, image_meta] + feature_maps)

    # --------------------------------------------------
    # OBJECT MASK SEGMENTATION

    # Conv layers
    mask_x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding="same"), name="mrcnn_mask_conv1")(roi_align_feat)
    mask_x = KL.TimeDistributed(BatchNorm(), name='mrcnn_mask_bn1')(mask_x, training=train_bn)
    mask_x = KL.Activation('relu')(mask_x)

    mask_x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding="same"), name="mrcnn_mask_conv2")(mask_x)
    mask_x = KL.TimeDistributed(BatchNorm(), name='mrcnn_mask_bn2')(mask_x, training=train_bn)
    mask_x = KL.Activation('relu')(mask_x)

    mask_x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding="same"), name="mrcnn_mask_conv3")(mask_x)
    mask_x = KL.TimeDistributed(BatchNorm(), name='mrcnn_mask_bn3')(mask_x, training=train_bn)
    mask_x = KL.Activation('relu')(mask_x)

    mask_x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding="same"), name="mrcnn_mask_conv4")(mask_x)
    mask_x = KL.TimeDistributed(BatchNorm(), name='mrcnn_mask_bn4')(mask_x, training=train_bn)
    mask_feat = KL.Activation('relu')(mask_x)

    # --------------------------------------------------
    # OBJECT EDGE SEGMENTATION

    # Conv layers
    edge_x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding="same"), name="obj_edge_mask_conv1")(roi_align_feat)
    edge_x = KL.TimeDistributed(BatchNorm(), name='obj_edge_mask_bn1')(edge_x, training=train_bn)
    edge_x = KL.Activation('relu')(edge_x)

    edge_x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding="same"), name="obj_edge_mask_conv2")(edge_x)
    edge_x = KL.TimeDistributed(BatchNorm(), name='obj_edge_mask_bn2')(edge_x, training=train_bn)
    edge_x = KL.Activation('relu')(edge_x)

    edge_x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding="same"), name="obj_edge_mask_conv3")(edge_x)
    edge_x = KL.TimeDistributed(BatchNorm(), name='obj_edge_mask_bn3')(edge_x, training=train_bn)
    edge_x = KL.Activation('relu')(edge_x)

    edge_x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding="same"), name="obj_edge_mask_conv4")(edge_x)
    edge_x = KL.TimeDistributed(BatchNorm(), name='obj_edge_mask_bn4')(edge_x, training=train_bn)
    edge_feat = KL.Activation('relu')(edge_x)

    # --------------------------------------------------
    # --------------------------------------------------
    # OBJECT MASK SEGMENTATION

    mask_x = KL.Add()([mask_feat, edge_feat])

    mask_x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding="same"), name="mrcnn_mask_conv5")(mask_x)
    mask_x = KL.TimeDistributed(BatchNorm(), name='mrcnn_mask_bn5')(mask_x, training=train_bn)
    mask_x = KL.Activation('relu')(mask_x)

    mask_x = KL.TimeDistributed(KL.Conv2DTranspose(256, (2, 2), strides=2, activation="relu"),
                                name="mrcnn_mask_deconv")(mask_x)
    mask_x = KL.TimeDistributed(KL.Conv2D(num_classes, (1, 1), strides=1, activation="sigmoid"),
                                name="mrcnn_mask")(mask_x)

    # --------------------------------------------------
    # OBJECT EDGE SEGMENTATION

    edge_sub = KL.Subtract()([edge_feat, mask_feat])

    edge_x = KL.Concatenate()([edge_feat, edge_sub])

    edge_x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding="same"), name="obj_edge_mask_conv5")(edge_x)
    edge_x = KL.TimeDistributed(BatchNorm(), name='obj_edge_mask_bn5')(edge_x, training=train_bn)
    edge_x = KL.Activation('relu')(edge_x)

    edge_x = KL.TimeDistributed(KL.Conv2DTranspose(256, (2, 2), strides=2, activation="relu"),
                                name="obj_edge_mask_deconv")(edge_x)
    edge_x = KL.TimeDistributed(KL.Conv2D(1, (1, 1), strides=1, activation="sigmoid"), name="obj_edge_mask")(edge_x)

    return mask_x, edge_x


def object_spatial_mask_module(in_obj_spatial_masks, config):
    # *********************** OBJECT SPATIAL MASKS ***********************
    obj_spa_mask = KL.TimeDistributed(KL.Conv2D(96, (5, 5), strides=2, padding="same"), name="obj_spatial_mask_conv_1")(
        in_obj_spatial_masks)
    obj_spa_mask = KL.TimeDistributed(BatchNorm(), name='obj_spatial_mask_bn_1')(obj_spa_mask, training=config.TRAIN_BN)
    obj_spa_mask = KL.Activation("relu")(obj_spa_mask)

    obj_spa_mask = KL.TimeDistributed(KL.Conv2D(128, (5, 5), strides=2, padding="same"),
                                      name="obj_spatial_mask_conv_2")(
        obj_spa_mask)
    obj_spa_mask = KL.TimeDistributed(BatchNorm(), name='obj_spatial_mask_bn_2')(obj_spa_mask, training=config.TRAIN_BN)
    obj_spa_mask = KL.Activation("relu")(obj_spa_mask)

    obj_spa_mask = KL.TimeDistributed(KL.Conv2D(64, (8, 8)), name="obj_spatial_mask_conv_3")(obj_spa_mask)
    obj_spa_mask = KL.TimeDistributed(BatchNorm(), name='obj_spatial_mask_bn_3')(obj_spa_mask, training=config.TRAIN_BN)
    obj_spa_mask = KL.Activation("relu")(obj_spa_mask)

    obj_spatial_mask_feat = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2), name="obj_spatial_mask_squeeze")(
        obj_spa_mask)

    return obj_spatial_mask_feat


def perform_attention_object_image(head_idx, proj_feat_size, obj_feat, img_feat, config):
    theta_dense_name = "head_caOI_" + str(head_idx) + "_theta_dense_1"
    theta_bn_name = "head_caOI_" + str(head_idx) + "_theta_bn_1"
    phi_dense_name = "head_caOI_" + str(head_idx) + "_phi_dense_1"
    phi_bn_name = "head_caOI_" + str(head_idx) + "_phi_bn_1"
    g_dense_name = "head_caOI_" + str(head_idx) + "_g_dense_1"
    g_bn_name = "head_caOI_" + str(head_idx) + "_g_bn_1"

    # Project features
    theta = KL.TimeDistributed(KL.Dense(proj_feat_size), name=theta_dense_name)(obj_feat)
    theta = KL.TimeDistributed(BatchNorm(), name=theta_bn_name)(theta, training=config.TRAIN_BN)
    phi = KL.Dense(proj_feat_size, name=phi_dense_name)(img_feat)
    phi = BatchNorm(name=phi_bn_name)(phi, training=config.TRAIN_BN)
    g_ft = KL.Dense(proj_feat_size, name=g_dense_name)(img_feat)
    g_ft = BatchNorm(name=g_bn_name)(g_ft, training=config.TRAIN_BN)

    # Repeat Vectors
    phi = KL.RepeatVector(config.TOP_K_DETECTION)(phi)
    g_ft = KL.RepeatVector(config.TOP_K_DETECTION)(g_ft)

    attn_name = "attn_caOI_layer_" + str(head_idx)
    attn = AttentionLayer(config, name=attn_name)([theta, phi, g_ft])

    return attn


def perform_attention_image_object(head_idx, proj_feat_size, obj_feat, img_feat, config):
    theta_dense_name = "head_caIO_" + str(head_idx) + "_theta_dense_1"
    theta_bn_name = "head_caIO_" + str(head_idx) + "_theta_bn_1"
    phi_dense_name = "head_caIO_" + str(head_idx) + "_phi_dense_1"
    phi_bn_name = "head_caIO_" + str(head_idx) + "_phi_bn_1"
    g_dense_name = "head_caIO_" + str(head_idx) + "_g_dense_1"
    g_bn_name = "head_caIO_" + str(head_idx) + "_g_bn_1"

    # Project features
    theta = KL.Dense(proj_feat_size, name=theta_dense_name)(img_feat)
    theta = BatchNorm(name=theta_bn_name)(theta, training=config.TRAIN_BN)
    phi = KL.TimeDistributed(KL.Dense(proj_feat_size), name=phi_dense_name)(obj_feat)
    phi = KL.TimeDistributed(BatchNorm(), name=phi_bn_name)(phi, training=config.TRAIN_BN)
    g_ft = KL.TimeDistributed(KL.Dense(proj_feat_size), name=g_dense_name)(obj_feat)
    g_ft = KL.TimeDistributed(BatchNorm(), name=g_bn_name)(g_ft, training=config.TRAIN_BN)

    # Repeat Vectors
    theta = KL.RepeatVector(config.TOP_K_DETECTION)(theta)

    attn_name = "attn_caIO_layer_" + str(head_idx)
    attn = AttentionLayer(config, name=attn_name)([theta, phi, g_ft])

    return attn


def selective_attention_module(num_heads, obj_feat, img_feat, config, mode):
    cross_attn_obj_img_head_outputs = []
    cross_attn_img_obj_head_outputs = []

    for h in range(num_heads):
        proj_feat_size = config.BOTTLE_NECK_SIZE // num_heads

        # Object - Image
        obj_img_attn = perform_attention_object_image(h, proj_feat_size, obj_feat, img_feat, config)
        cross_attn_obj_img_head_outputs.append(obj_img_attn)

        # Image - Object
        img_obj_attn = perform_attention_image_object(h, proj_feat_size, obj_feat, img_feat, config)
        cross_attn_img_obj_head_outputs.append(img_obj_attn)

    # Concatenate Heads
    cross_attn_obj_img = KL.Concatenate()(cross_attn_obj_img_head_outputs) if num_heads > 1 \
        else cross_attn_obj_img_head_outputs[0]

    cross_attn_img_obj = KL.Concatenate()(cross_attn_img_obj_head_outputs) if num_heads > 1 \
        else cross_attn_img_obj_head_outputs[0]

    # Linear
    cross_attn_obj_img = KL.TimeDistributed(KL.Dense(config.BOTTLE_NECK_SIZE),
                                            name='obj_caOI_feat_dense_1')(cross_attn_obj_img)
    cross_attn_obj_img = KL.TimeDistributed(BatchNorm(),
                                            name='obj_caOI_feat_bn_1')(cross_attn_obj_img, training=config.TRAIN_BN)

    cross_attn_img_obj = KL.TimeDistributed(KL.Dense(config.BOTTLE_NECK_SIZE),
                                            name='obj_caIO_feat_dense_1')(cross_attn_img_obj)
    cross_attn_img_obj = KL.TimeDistributed(BatchNorm(),
                                            name='obj_caIO_feat_bn_1')(cross_attn_img_obj, training=config.TRAIN_BN)

    final_attn = KL.Multiply()([cross_attn_obj_img, cross_attn_img_obj])

    final_attn = KL.Add()([final_attn, obj_feat])

    # Feed_forward
    final_attn = KL.TimeDistributed(KL.Dense(config.BOTTLE_NECK_SIZE), name="obj_attn_feat_ff_dense_1")(final_attn)
    final_attn = KL.TimeDistributed(BatchNorm(), name='obj_attn_feat_ff_bn_1')(final_attn, training=config.TRAIN_BN)
    final_attn = KL.Activation('relu')(final_attn)

    if mode == "training":
        dropout = 0.5
        final_attn = KL.TimeDistributed(KL.Dropout(dropout))(final_attn)

    return final_attn
